# GitLab Runner on Kubernetes: A Complete Guide
## Part 2 â€” Deep Dive into Container Architecture

---

## About This Series

This series documents my journey of learning and deploying GitLab Runner on Kubernetes, from complete beginner to production deployment. What makes this unique is that **I learned and implemented this alongside Claude Code** â€” an AI coding assistant that helped me:

- Navigate complex Kubernetes networking concepts
- Debug DNS resolution issues in containerized environments
- Understand the four-container architecture of GitLab Runner
- Build production-ready configurations

**Why share this?** Because AI-assisted learning accelerated my understanding dramatically. Concepts that would typically take weeks to grasp through trial-and-error became clear in days through:
- Real-time explanations of error messages
- Step-by-step debugging of network issues
- Detailed breakdowns of container interactions
- Architecture visualizations and timing diagrams

If you're exploring unfamiliar technologies (like I was with GitLab on K8s), this series demonstrates how AI can be a powerful learning companion â€” not by giving you copy-paste solutions, but by helping you **understand deeply** as you build.

**Learning Tips**: As you read through this series, you'll encounter many technical concepts â€” Kubernetes networking, Docker-in-Docker, DNS resolution, container orchestration, and more. Don't hesitate to use AI assistants (like Claude, ChatGPT, or others) alongside this guide to:
- Explain unfamiliar terms and acronyms in context
- Break down complex concepts into digestible explanations
- Generate examples that match your specific use case
- Ask "why" questions about architectural decisions
- Debug errors you encounter in your own setup

The best learning happens through active engagement. Use AI tools to explore concepts at your own pace and depth.

> *Read [Part 1](link-to-part-1) first if you haven't already â€” we cover the basics and get a working runner deployed.*

---

In Part 1, we deployed a GitLab Runner on Kubernetes and ran our first pipeline. We briefly mentioned the "four containers" but didn't explore what's actually happening inside each one.

In this part, we'll dissect the complete architecture. By the end, you'll understand:
- Why there are four different containers and what each one does
- How Docker-in-Docker really works (spoiler: it's client-server, not magic)
- The complete lifecycle from "git push" to "pipeline success"
- Resource management and what happens when jobs run concurrently

Let's start with the architecture overview.

---

## The Four-Container Architecture

When you trigger a CI/CD job on Kubernetes executor, GitLab Runner creates a Pod with **up to four specialized containers**. Each has a distinct role:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Runner Manager Pod (Persistent)                   â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚  gitlab-runner:v18.3.0                       â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - Polls GitLab for jobs                     â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - Creates Job Pods when work arrives        â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - Monitors job execution                    â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â”‚ Creates on-demand                 â”‚
â”‚                          â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Job Pod (Ephemeral)                               â”‚     â”‚
â”‚  â”‚                                                    â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚
â”‚  â”‚  â”‚  1. Helper (Init Container)                â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     gitlab-runner-helper:v18.3.0           â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     - Clones Git repository                â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     - Downloads artifacts                  â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     - Uploads results after job            â”‚   â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚
â”‚  â”‚                                                    â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚
â”‚  â”‚  â”‚  2. Build (Main Container)                 â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     User-defined image (e.g., docker:latest)â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     - Executes CI/CD script                â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     - Sends commands to services           â”‚   â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚
â”‚  â”‚                                                    â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚
â”‚  â”‚  â”‚  3. Service (Sidecar Container)            â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     User-defined (e.g., docker:dind)       â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     - Provides background services         â”‚   â”‚     â”‚
â”‚  â”‚  â”‚     - Runs entire job duration             â”‚   â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Let's examine each container in detail.

---

## Container 1: Runner Manager Pod

**Image**: `gitlab/gitlab-runner:v18.3.0`
**Type**: Deployment (persistent)
**Lifespan**: Runs continuously

### Responsibilities

The Runner Manager is the "orchestrator" that never stops running:

1. **Job Polling**: Continuously queries GitLab API for pending jobs
2. **Pod Creation**: Creates Job Pods in Kubernetes when work arrives
3. **Status Monitoring**: Tracks job execution and reports back to GitLab
4. **Log Collection**: Streams job logs to GitLab UI in real-time
5. **Cleanup**: Ensures completed Job Pods are removed

### Configuration Example

```yaml
# From gitlab-runner-values.yaml
replicas: 3              # 3 manager pods for HA
concurrent: 10           # Each manager handles 10 jobs max
```

**Capacity calculation**:
- 3 managers Ã— 10 concurrent = **30 jobs can run simultaneously**

### How It Works

```
Manager Pod Main Loop:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ while true:                             â”‚
â”‚   jobs = poll_gitlab_api()              â”‚
â”‚   for job in jobs:                      â”‚
â”‚     if capacity_available():            â”‚
â”‚       pod = create_job_pod(job)         â”‚
â”‚       monitor_pod_status(pod)           â”‚
â”‚       stream_logs_to_gitlab(pod)        â”‚
â”‚   sleep(3 seconds)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The manager itself doesn't execute any CI/CD scripts â€” it's purely a control plane component.

---

## Container 2: Helper Container

**Image**: `gitlab-runner-helper:x86_64-v18.3.0`
**Type**: Init Container
**Lifespan**: Runs at job start and end

### Responsibilities

The Helper handles all Git and artifact operations:

**At Job Start**:
- Clones the Git repository to a shared volume
- Downloads artifacts from previous stages
- Sets up the working directory

**At Job End**:
- Uploads artifacts to GitLab
- Uploads cache (if configured)
- Uploads test reports

### Execution Flow

```
T+0s   Job Pod created
       â”‚
T+1s   Helper Container starts (as init container)
       â”‚
       â”œâ”€ Step 1: Clone repository
       â”‚  $ git clone --depth 50 https://gitlab.com/user/repo.git
       â”‚
       â”œâ”€ Step 2: Checkout specific commit
       â”‚  $ git checkout $CI_COMMIT_SHA
       â”‚
       â”œâ”€ Step 3: Download artifacts (if any)
       â”‚  $ wget $GITLAB_API/jobs/$JOB_ID/artifacts
       â”‚
       â””â”€ Step 4: Complete (exit)

T+3s   Helper exits, Build Container can start

       [Build Container runs...]

T+160s Build completes

       Helper reactivates (as post-job container)
       â”‚
       â”œâ”€ Upload artifacts
       â”‚  $ curl -F "file=@output.zip" $GITLAB_API/artifacts
       â”‚
       â””â”€ Upload cache
          $ curl -F "file=@.cache.tar.gz" $GITLAB_API/cache
```

### Shared Volume

The Helper and Build containers share a volume:

```
Shared emptyDir Volume: /builds
â”œâ”€â”€ namespace/
â”‚   â””â”€â”€ project-name/
â”‚       â”œâ”€â”€ .git/
â”‚       â”œâ”€â”€ src/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ .gitlab-ci.yml
```

**Key point**: The Build Container doesn't clone code itself â€” it just uses what Helper prepared.

---

## Container 3: Build Container

**Image**: Defined by `image` in `.gitlab-ci.yml`
**Type**: Main container
**Lifespan**: Runs for the duration of the job

### Responsibilities

This is where your CI/CD script actually executes:

- Runs `before_script`, `script`, `after_script`
- Has access to the cloned repository
- Communicates with Service Containers via network
- Reports exit code back to GitLab

### Configuration

In your `.gitlab-ci.yml`:

```yaml
build-job:
  image: docker:latest      # â† Defines Build Container image
  script:
    - docker build -t myapp .
    - docker push myapp
```

The Build Container runs with these defaults:
- Working directory: `/builds/<namespace>/<project>`
- User: `root` (configurable)
- Network: Shared with Service Containers

### Resource Limits

From `gitlab-runner-values.yaml`:

```yaml
[runners.kubernetes]
  cpu_limit = "2"              # Max 2 CPU cores
  memory_limit = "2Gi"         # Max 2GB RAM
  cpu_request = "500m"         # Guaranteed 0.5 cores
  memory_request = "512Mi"     # Guaranteed 512MB
```

What happens if exceeded?
- **CPU limit**: Throttled (job slows down)
- **Memory limit**: OOM killed (job fails)

---

## Container 4: Service Container (Docker-in-Docker)

**Image**: Defined by `services` in `.gitlab-ci.yml`
**Type**: Sidecar container
**Lifespan**: Runs entire job duration

This is the most complex container, so let's break it down thoroughly.

### What is Docker-in-Docker?

When your CI/CD job needs to build Docker images, you face a challenge: **the Build Container is already inside Docker** (running on Kubernetes). How do you run Docker inside Docker?

**Solution**: Run a complete Docker daemon (`dockerd`) in a separate container.

### Configuration Example

```yaml
# .gitlab-ci.yml
services:
  - name: docker:dind
    alias: docker          # â† Network alias (important!)
    command:
      - "--tls=false"      # Disable TLS for simplicity
      - "--insecure-registry=my-registry.com"

variables:
  DOCKER_HOST: tcp://docker:2375  # â† Connect to DinD
```

### Complete Lifecycle (0s to 170s)

Let me walk you through the entire lifecycle with actual timing:

```
T+0s    Job Pod created by Runner Manager
        Kubernetes scheduler selects a node

T+1s    ========== Phase 1: Container Startup ==========

        Helper Container (init) starts
        â””â”€ Clones code (2s)

T+3s    Helper completes, enters Waiting state

        ========== Phase 2: Service Container Startup ==========

T+4s    DinD Container starts
        â””â”€ Executes ENTRYPOINT: dockerd-entrypoint.sh
            â”œâ”€ Initialize storage driver (overlay2)
            â”œâ”€ Set up iptables rules
            â”œâ”€ Start containerd
            â””â”€ Start dockerd daemon

T+5s    dockerd starting...
        Logs:
          level=info msg="Starting up"
          level=info msg="containerd not running, starting managed containerd"

T+7s    dockerd listening on ports
        Logs:
          level=info msg="API listen on /var/run/docker.sock"
          level=info msg="API listen on [::]:2375"

T+8s    ========== Phase 3: DinD Ready ==========

        DinD fully ready, waiting for client connections
        Status: Ready = true

        Build Container starts
        â””â”€ Executes before_script

T+9s    ========== Phase 4: First Request ==========

        Build Container: docker info

        Network flow:
          Build Container (docker CLI)
              â†“ TCP connection
          tcp://docker:2375
              â†“ Resolves via alias
          127.0.0.1:2375 (localhost)
              â†“ Local loopback
          DinD Container (dockerd process)

        DinD logs:
          level=debug msg="Calling GET /v1.40/info"

        Returns: Server Version: 27.1.1
        âœ… Connection confirmed

T+10s   ========== Phase 5: Execute Build ==========

        Build Container: docker build -t myapp .

        Request flow:
          â‘  Build sends build context (tar file)
             â†“
          â‘¡ DinD receives context, extracts to temp dir
             Logs: "Building context from tarball"
             â†“
          â‘¢ DinD reads Dockerfile
             Parsing: FROM python:3.9-slim
             â†“
          â‘£ DinD checks local image cache
             Result: Not found
             â†“
          â‘¤ DinD performs DNS resolution
             Query: /etc/resolv.conf
             Nameserver: 10.0.0.2
             Domain: registry.example.com
             Result: 203.0.113.10
             â†“
          â‘¥ DinD establishes HTTPS connection
             Connect: 203.0.113.10:443
             â†“
          â‘¦ DinD pulls image layers
             Logs:
               "Pulling from library/python"
               "Layer 1: Downloading [=>  ] 5MB/50MB"
               "Layer 2: Downloading [===>] 15MB/30MB"
             (35s download time)
             â†“
          â‘§ DinD executes Dockerfile instructions
             COPY . . â†’ Copy code into image
             RUN pip install â†’ Execute install command
             â†“
          â‘¨ DinD creates new image
             Image ID: sha256:abc123...
             Tag: myapp:latest
             â†“
          â‘© DinD returns build result
             â†“
          Build Container receives success message
             "Successfully built abc123..."
             "Successfully tagged myapp:latest"

T+120s  ========== Phase 6: Push Image ==========

        Build Container: docker push myapp:latest

        Push flow:
          â‘  Build sends push request
             â†“
          â‘¡ DinD reads image layers
             Check which layers exist on remote
             â†“
          â‘¢ DinD connects to target registry
             Address: private-registry.example.com
             Auth: Uses docker login credentials
             â†“
          â‘£ DinD uploads image layers
             Logs:
               "Layer 1: Pushed (already exists)"
               "Layer 2: Pushed (already exists)"
               "Layer 3: Pushing [=>  ] 10MB/50MB"
             â†“
          â‘¤ DinD pushes manifest
             Tag: 56cc4792-86
             â†“
          â‘¥ DinD returns success
             â†“
          Build Container receives success
             "56cc4792-86: digest: sha256:def456... size: 1234"

T+160s  ========== Phase 7: Build Script Completes ==========

        Build Container finishes script execution
        Container enters Completed state

        DinD Container still running
        â””â”€ Waiting for Pod termination signal

T+165s  ========== Phase 8: Cleanup ==========

        Kubernetes sends SIGTERM to all containers

        DinD receives signal
        Logs:
          level=info msg="Processing signal 'terminated'"
          level=info msg="Stopping daemon"
          level=info msg="Daemon shutdown complete"

        DinD graceful shutdown (5s)
        â””â”€ Stop containerd
        â””â”€ Clean temporary files
        â””â”€ Release ports

T+170s  Pod deletion complete
```

---

## The Critical Concept: Docker Client-Server Architecture

This is the most commonly misunderstood aspect. Let me clarify:

### What Actually Happens

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Job Pod                             â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Build Container  â”‚         â”‚  DinD Container  â”‚    â”‚
â”‚  â”‚                  â”‚         â”‚                  â”‚    â”‚
â”‚  â”‚ âœ… Where you     â”‚         â”‚ âœ… Where work    â”‚    â”‚
â”‚  â”‚ type commands    â”‚         â”‚ actually happens â”‚    â”‚
â”‚  â”‚ (Client)         â”‚         â”‚ (Server)         â”‚    â”‚
â”‚  â”‚                  â”‚         â”‚                  â”‚    â”‚
â”‚  â”‚ $ docker build   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  dockerd         â”‚    â”‚
â”‚  â”‚   â†‘              â”‚  API    â”‚   â†“              â”‚    â”‚
â”‚  â”‚   Just CLI tool  â”‚  Call   â”‚   Build engine   â”‚    â”‚
â”‚  â”‚                  â”‚         â”‚                  â”‚    â”‚
â”‚  â”‚ Does:            â”‚         â”‚ Does:            â”‚    â”‚
â”‚  â”‚ - Read Dockerfileâ”‚         â”‚ - Pull images    â”‚    â”‚
â”‚  â”‚ - Package contextâ”‚         â”‚ - Extract contextâ”‚    â”‚
â”‚  â”‚ - Send API req   â”‚         â”‚ - Run commands   â”‚    â”‚
â”‚  â”‚ - Display logs   â”‚         â”‚ - Create layers  â”‚    â”‚
â”‚  â”‚                  â”‚         â”‚ - Generate IDs   â”‚    â”‚
â”‚  â”‚ Doesn't:         â”‚         â”‚ Doesn't:         â”‚    â”‚
â”‚  â”‚ - Pull images    â”‚         â”‚ - Read user inputâ”‚    â”‚
â”‚  â”‚ - Run commands   â”‚         â”‚ - Display output â”‚    â”‚
â”‚  â”‚ - Create layers  â”‚         â”‚                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Responsibilities

| Step | Build Container Does | DinD Container Does | Where? |
|------|---------------------|---------------------|--------|
| **1. Start command** | Execute `docker build -t myapp .` | dockerd listens on 2375 | Build |
| **2. Preparation** | Read Dockerfile in current dir | Wait for requests | Build |
| **3. Package context** | Create tar file of current dir (5MB) | - | Build |
| **4. Send request** | HTTP POST tar to `tcp://docker:2375/build` | Receive tar, extract to `/tmp/docker-build-xxx` | Build â†’ DinD |
| **5. Parse Dockerfile** | - | Read Dockerfile, parse instructions | DinD |
| **6. Pull images** | - | Execute `FROM python:3.9-slim`<br>DNS resolution, image pull (35s) | DinD |
| **7. Build layers** | Receive and display log stream | Execute `RUN pip install`<br>Run command in temporary container | DinD |
| **8. Complete** | Display "Successfully built abc123" | Commit image layers, generate image ID | DinD â†’ Build |

### Why This Design?

**1. Separation of Concerns**
- Build Container: User interaction, script execution
- DinD Container: Docker engine, image management

**2. Resource Isolation**
- Build failure doesn't crash the Docker daemon
- You can limit DinD resources independently

**3. Security**
- Build Container doesn't need privileged mode
- Only DinD needs `privileged: true`
- Reduces security risk surface

**4. Flexibility**
- Can connect to remote Docker daemons
- Can swap build tools without touching services

### Verification

You can verify this architecture:

**In Build Container**:
```bash
# Only has docker CLI tool
$ which docker
/usr/local/bin/docker

# It's just a client binary
$ file /usr/local/bin/docker
/usr/local/bin/docker: ELF 64-bit LSB executable

# No dockerd daemon
$ which dockerd
(not found)

# Connects to remote dockerd
$ echo $DOCKER_HOST
tcp://docker:2375

# No dockerd process
$ ps aux | grep dockerd
(no results)
```

**In DinD Container**:
```bash
# Has dockerd daemon running
$ ps aux | grep dockerd
root  21  dockerd --host=tcp://0.0.0.0:2375 --tls=false

# Daemon listening on port
$ netstat -tuln | grep 2375
tcp6  0  0  :::2375  :::*  LISTEN
```

---

## Container Communication

All containers in the Job Pod share the same network namespace:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Job Pod Network Namespace                  â”‚
â”‚           (All containers share this)                â”‚
â”‚                                                      â”‚
â”‚  Network Interfaces:                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  lo (loopback)                             â”‚     â”‚
â”‚  â”‚  127.0.0.1/8                               â”‚     â”‚
â”‚  â”‚  Used for: Container-to-container (docker) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  eth0 (veth pair)                          â”‚     â”‚
â”‚  â”‚  172.24.1.100/24 (example)                 â”‚     â”‚
â”‚  â”‚  Used for: External communication          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                      â”‚
â”‚  /etc/hosts:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  127.0.0.1 localhost                       â”‚     â”‚
â”‚  â”‚  127.0.0.1 docker    # Service alias       â”‚     â”‚
â”‚  â”‚  172.24.1.100 runner-xxx-xxx               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key points**:
- Service alias (`docker`) resolves to `127.0.0.1`
- Communication happens via localhost (no network overhead)
- All containers see the same IP address and network interfaces

---

## Resource Management

Understanding resource allocation is crucial for stable operations.

### Per-Job Resources

Each Job Pod can consume:

```yaml
# Build Container
cpu_limit = "2"              # Max 2 cores
memory_limit = "2Gi"         # Max 2GB

# Service Container (DinD)
service_cpu_limit = "2"      # Max 2 cores
service_memory_limit = "2Gi" # Max 2GB

# Helper Container
helper_cpu_limit = "500m"    # Max 0.5 cores
helper_memory_limit = "512Mi" # Max 512MB
```

**Total per job**: Up to 4.5 CPU cores and 4.5GB memory

### Cluster Capacity Planning

Example calculation:

```
Cluster: 3 worker nodes, each with 8 cores and 32GB RAM

Available for jobs:
- CPU: 3 Ã— 8 = 24 cores (assuming 100% allocatable)
- Memory: 3 Ã— 32GB = 96GB

Maximum concurrent jobs (if all use max resources):
- By CPU: 24 Ã· 4.5 = 5 jobs
- By memory: 96 Ã· 4.5 = 21 jobs
- Bottleneck: CPU â†’ 5 concurrent jobs max

Actual capacity (with typical 50% CPU usage):
- ~10 concurrent jobs comfortably
```

### What Happens When Limits Are Exceeded?

**CPU Limit**:
```
Job tries to use 3 cores (limit is 2)
   â†“
Kubernetes throttles the container
   â†“
Job runs slower but doesn't fail
   â†“
Visible in metrics: CPU throttling
```

**Memory Limit**:
```
Job tries to allocate 3GB (limit is 2GB)
   â†“
Container OOM killed immediately
   â†“
Pod restarts (if restart policy allows)
   â†“
Job fails with: "OOMKilled: Container exceeded memory limit"
```

---

## Putting It All Together

Let's trace a complete job from start to finish:

```
Developer: git push

GitLab Server: Pipeline created
   â†“
Runner Manager: Polls API, sees new job
   â†“
Runner Manager: Creates Job Pod
   â†“
Kubernetes: Schedules Pod to worker-node-2
   â†“
Helper Container: Starts (init)
   - Clones repository
   - Downloads artifacts
   - Exits
   â†“
Service Container (DinD): Starts
   - Starts dockerd
   - Listens on tcp://0.0.0.0:2375
   - Ready
   â†“
Build Container: Starts
   - Reads /builds/namespace/project/
   - Executes before_script
   - Executes script:
     * docker build
        â†’ Connects to tcp://docker:2375
        â†’ DinD performs build
        â†’ Returns success
     * docker push
        â†’ Connects to tcp://docker:2375
        â†’ DinD pushes image
        â†’ Returns success
   - Exits with code 0
   â†“
Helper Container: Reactivates
   - Uploads artifacts
   - Uploads cache
   - Exits
   â†“
Runner Manager: Collects logs
   - Streams to GitLab
   - Marks job as success
   â†“
Kubernetes: Deletes Job Pod
   â†“
GitLab: Shows âœ… Pipeline passed
```

**Total time**: ~160 seconds (varies by image size and network)

---

## Key Takeaways

**Container Roles**:
1. **Runner Manager**: Orchestrator (never executes jobs)
2. **Helper**: Git and artifact handler
3. **Build**: Script executor (your code runs here)
4. **Service (DinD)**: Background services (actual work happens here)

**Docker-in-Docker Architecture**:
- Build Container = Docker **client** (sends commands)
- DinD Container = Docker **server** (does the work)
- Communication via TCP on localhost

**Resource Management**:
- Each job can use up to 4.5 CPU cores and 4.5GB RAM (with default limits)
- CPU limits cause throttling, memory limits cause OOM kills
- Plan cluster capacity based on concurrent job requirements

**Network**:
- All containers share a Pod network namespace
- Service aliases resolve to 127.0.0.1
- No network overhead for inter-container communication

---

## What's Next?

In Part 2, we explored the internal architecture and timing of each container. We now understand how the pieces fit together.

**Coming up in Part 3**:
- ğŸ¬ **Complete CI/CD workflow** â€” From code push to production deployment
- ğŸ“ **.gitlab-ci.yml best practices** â€” Build, test, and deploy stages
- ğŸ³ **Docker image optimization** â€” Faster builds, smaller images
- â˜¸ï¸ **Kubernetes deployment** â€” Using kubectl from CI/CD

**Preview**: We'll build a real-world pipeline that builds a Docker image, pushes it to a registry, and deploys it to Kubernetes â€” all automated.

---

## Practice Exercises

**Exercise 1**: Calculate your cluster's job capacity

Given:
- 4 worker nodes
- 16 cores and 64GB RAM per node
- Job limits: 2 CPU, 2GB memory

How many concurrent jobs can you run?

**Exercise 2**: Monitor container resource usage

```bash
# Watch Job Pod resources in real-time
kubectl top pod -n gitlab --containers

# Compare to limits
kubectl get pod <job-pod> -n gitlab -o yaml | grep -A 5 resources
```

**Exercise 3**: Inspect DinD container

```bash
# While a job is running, exec into DinD
kubectl exec -n gitlab <job-pod> -c docker -- ps aux

# Check dockerd listening ports
kubectl exec -n gitlab <job-pod> -c docker -- netstat -tuln | grep 2375
```

---

## Resources

- [GitLab Runner Executors Documentation](https://docs.gitlab.com/runner/executors/)
- [Kubernetes Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Docker Engine API Reference](https://docs.docker.com/engine/api/)

---

**Part 3 coming soon** â€” we'll build a complete CI/CD pipeline with Docker builds and Kubernetes deployments.

If you found this helpful, feel free to share it with others learning GitLab CI/CD.

---

*This is Part 2 of a 5-part series on GitLab Runner on Kubernetes, documenting the learning and implementation process.*

**Series index**:
- [Part 1: Architecture & Quick Setup](link-to-part-1)
- **Part 2: Deep Dive into Container Architecture** â† You are here
- Part 3: Building a Real-World CI/CD Pipeline (coming soon)
- Part 4: Solving DNS and Network Issues (coming soon)
- Part 5: Production Best Practices (coming soon)

---

*Tags: #GitLab #Kubernetes #CICD #DevOps #Docker #ContainerArchitecture*
