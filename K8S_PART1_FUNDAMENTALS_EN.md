# Master Kubernetes in Hours with AI: Part 1 - From Zero to Understanding Core Architecture

## How This Tutorial Was Created with AI

*This comprehensive Kubernetes tutorial series was created through an innovative collaboration with AI, demonstrating how modern developers can rapidly master complex technologies. In just a few hours of interaction with Claude AI, I transformed from a Kubernetes beginner to having production-ready knowledge - and you can too.*

**The AI-Accelerated Learning Approach:**
- ğŸ¤– **Structured Knowledge Extraction**: AI helped organize scattered Kubernetes concepts into a logical learning path
- ğŸ’¡ **Interactive Q&A Learning**: Complex topics were clarified through continuous dialogue, just like having a senior engineer beside you
- ğŸš€ **Practice-Driven Content**: Every concept includes hands-on examples generated and tested with AI assistance
- ğŸ“š **Comprehensive Yet Concise**: AI helped distill years of community knowledge into hours of focused learning

**What Makes This Different:**
Traditional documentation can be overwhelming and fragmented. By leveraging AI, this tutorial provides a curated, progressive learning experience that adapts to real-world needs. Each section answers the "why" before the "how," ensuring you understand not just commands, but the reasoning behind Kubernetes design choices.

**Your Learning Journey:**
Part 1 establishes the foundation - core concepts and architecture. Part 2 delivers hands-on deployment skills. Part 3 prepares you for production. All created through iterative AI collaboration, saving you hundreds of hours of research.

*Ready to experience the future of technical learning? Let's dive in.*

---

## Part 1 Overview

Welcome to Part 1 of mastering Kubernetes with AI! This part will establish your core knowledge foundation of Kubernetes.

In this part, you will learn:
- **What is Kubernetes** - Understanding the core concepts and value of container orchestration
- **Kubernetes Architecture Design** - Mastering cluster components and working principles
- **Core Resource Objects** - Deep understanding of Pod, Service, Deployment and other fundamental concepts

This foundational knowledge is the cornerstone for subsequent hands-on operations. We recommend reading in order and practicing each concept when possible.

**Target Audience**:
- Developers with basic Docker container experience
- Operations personnel interested in container orchestration technology
- Technical teams preparing to use Kubernetes in production environments

**Estimated Reading Time**: 2-3 hours

---

## Table of Contents

1. [What is Kubernetes](#1-what-is-kubernetes)
2. [Core Architecture](#2-core-architecture)
3. [Core Resource Objects Deep Dive](#3-core-resource-objects-deep-dive)

---

## 1. What is Kubernetes

### 1.1 Understanding K8s in One Sentence
**Kubernetes is a container orchestration platform** that automates deployment, scaling, and management of containerized applications.

### 1.2 What Problems It Solves

| Traditional Deployment Pain Points | K8s Solutions |
|-----------------------------------|---------------|
| Manual application deployment to servers | Declarative configuration, automated deployment |
| Server failures require manual migration | Automatic failover, self-healing capabilities |
| Traffic increases require manual scaling | Auto-scaling (HPA/VPA) |
| Application updates require downtime | Rolling updates, zero-downtime deployment |
| Multi-environment configuration chaos | Unified management with ConfigMap/Secret |
| Service discovery depends on hardcoding | DNS service discovery, load balancing |

### 1.3 K8s vs Docker

```
Docker: Package and run individual containers
K8s: Orchestrate and manage thousands of containers

Analogy:
- Docker = Container (standardized packaging)
- K8s = Port Management System (scheduling, storage, transportation)
```

---

## 2. Core Architecture

### 2.1 Cluster Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                   â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Control Plane â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                  â”‚  â”‚
â”‚  â”‚  API Server â†â†’ etcd                             â”‚  â”‚
â”‚  â”‚      â†•                                          â”‚  â”‚
â”‚  â”‚  Scheduler  Controller Manager  Cloud Provider  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Worker Nodes                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Node 1    â”‚  â”‚   Node 2    â”‚  â”‚  Node N  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚          â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ kubelet â”‚ â”‚  â”‚ â”‚ kubelet â”‚ â”‚  â”‚ kubelet  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ kube-   â”‚ â”‚  â”‚ â”‚ kube-   â”‚ â”‚  â”‚ kube-    â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ proxy   â”‚ â”‚  â”‚ â”‚ proxy   â”‚ â”‚  â”‚ proxy    â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ docker  â”‚ â”‚  â”‚ â”‚ docker  â”‚ â”‚  â”‚ docker   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚          â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   [Pods]    â”‚  â”‚   [Pods]    â”‚  â”‚  [Pods]  â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Control Plane Components

| Component | Function | Analogy |
|-----------|----------|---------|
| **API Server** | Entry point for all operations, REST API | Front desk receptionist |
| **etcd** | Stores all cluster data | Database |
| **Scheduler** | Decides which node runs Pods | Dispatcher |
| **Controller Manager** | Ensures actual state matches desired state | Supervisor |

### 2.3 Node Components

| Component | Function | Running Location |
|-----------|----------|------------------|
| **kubelet** | Manages Pod lifecycle | Every node |
| **kube-proxy** | Network proxy, load balancing | Every node |
| **Container Runtime** | Runs containers (Docker/containerd) | Every node |

---

## 3. Core Resource Objects Deep Dive

### 3.1 Workload Resources

#### Pod - Minimum Deployment Unit
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
```
**Characteristics**:
- Contains one or more containers
- Shares network and storage
- Short-lived lifecycle
- Usually not created directly

#### Deployment - Stateless Application Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3              # Number of replicas
  strategy:
    type: RollingUpdate    # Rolling update strategy
    rollingUpdate:
      maxSurge: 1          # Maximum 1 extra Pod during update
      maxUnavailable: 1    # Maximum 1 Pod unavailable during update
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: app
        image: myapp:v2
        resources:         # Resource limits
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
```

#### StatefulSet - Stateful Applications
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
  volumeClaimTemplates:    # Persistent storage template
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```
**Characteristics**:
- Stable network identities (mysql-0, mysql-1)
- Ordered deployment and deletion
- Persistent storage

#### DaemonSet - Daemon Processes
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      containers:
      - name: node-exporter
        image: prom/node-exporter
```
**Use Cases**: Run one Pod per node (log collection, monitoring agents)

#### Job/CronJob - Task Scheduling
```yaml
# One-time task
apiVersion: batch/v1
kind: Job
metadata:
  name: backup
spec:
  template:
    spec:
      containers:
      - name: backup
        image: backup-tool
        command: ["./backup.sh"]
      restartPolicy: OnFailure

---
# Scheduled task
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
spec:
  schedule: "0 2 * * *"  # Execute at 2 AM daily
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: backup-tool
          restartPolicy: OnFailure
```

### 3.2 Service Discovery and Load Balancing

#### Why Do We Need Services?

**Problem Scenario**:
```
Application A needs to call Application B's API:
- Pod IPs are dynamic (change on restart)
- Multiple B Pod replicas exist (which one to call?)
- Pods may scale up/down anytime
```

**Service Solution**:
```
Application A â†’ Service B (fixed entry point) â†’ Load Balancing â†’ Pod B1, B2, B3
```

#### How Services Work

**Service Discovery Mechanism**:
```
What happens when creating a Service:

1. Assign ClusterIP (virtual IP)
   Service: my-service â†’ 10.96.10.15

2. Create DNS record
   my-service.default.svc.cluster.local â†’ 10.96.10.15

3. Create Endpoints (endpoint list)
   Monitor Pods matching selector, maintain IP list
   Endpoints: [10.1.0.5:8080, 10.1.0.6:8080, 10.1.0.7:8080]

4. Configure iptables/IPVS rules
   Implement load balancing to backend Pods
```

**Load Balancing Implementation**:
```
Client request flow:

1. Pod A: curl http://my-service:8080
                â†“
2. DNS resolution: my-service â†’ 10.96.10.15
                â†“
3. Request sent to: 10.96.10.15:8080
                â†“
4. kube-proxy intercepts (iptables rules)
                â†“
5. Load balancing selection: RoundRobin
                â†“
6. Forward to actual Pod: 10.1.0.6:8080
```

#### Service Types Deep Dive

**1. ClusterIP (Default Type)**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  type: ClusterIP  # Can be omitted
  selector:
    app: backend
  ports:
  - port: 80        # Service port
    targetPort: 8080 # Pod port
```

Characteristics and Use Cases:
- **Access Scope**: Cluster internal only
- **What You Get**: Virtual IP + DNS name
- **Access Methods**:
  - Short name: `backend` (same namespace)
  - Full domain: `backend.default.svc.cluster.local`
  - ClusterIP: `10.96.10.15`
- **Use Cases**: Internal microservices, databases, caches

Practical Usage:
```python
# In Pod application code
import requests
# Recommended: Use Service name
response = requests.get('http://backend/api/data')
```

**2. NodePort**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - port: 80         # Service port
    targetPort: 8080  # Pod port
    nodePort: 30080   # Node port (30000-32767)
```

Characteristics and Workflow:
- **Access Scope**: Cluster external + internal
- **Exposure Method**: Specified port on each node
- **Access Address**: `<any-node-ip>:30080`
- **Use Cases**: Development testing, private cloud, simple external services

```
External client
    â†“
Node IP:30080 (any node works)
    â†“
kube-proxy forwarding
    â†“
Service (internal load balancing)
    â†“
Pod (final request processing)
```

**3. LoadBalancer**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  type: LoadBalancer
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 8080
```

Characteristics and Actual Effects:
- **Prerequisites**: Cloud provider support (AWS ELB, Alibaba Cloud SLB, etc.)
- **What You Get**: External load balancer + public IP
- **Access Method**: `<LoadBalancer-IP>:80`
- **Use Cases**: Production environments, public services

```bash
kubectl get svc frontend
NAME       TYPE           EXTERNAL-IP                 PORT(S)
frontend   LoadBalancer   a1b2c3d4.elb.amazonaws.com  80:31234/TCP
```

**4. ExternalName**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: database
spec:
  type: ExternalName
  externalName: mysql.example.com
```

Characteristics and Use Cases:
- **Function**: Create internal alias for external services
- **Implementation**: CNAME record
- **No Selector**: Not associated with Pods
- **Use Cases**: External databases, cross-cluster services, migration transition

Usage Example:
```python
# Use within application - uniform Service name access regardless of database location
conn = mysql.connect(host='database', port=3306)
# Actually resolves to: mysql.example.com
```

#### Ingress - Layer 7 Routing

**Why Do We Need Ingress?**

Service Limitations:
- LoadBalancer requires one public IP per service (expensive)
- NodePort port management chaos
- Cannot route based on domain/path
- No SSL/TLS termination support

Ingress Solution:
```
                 Ingress Controller
                         â†“
    app.com/web â†’ web-service â†’ web-pods
    app.com/api â†’ api-service â†’ api-pods
    admin.com   â†’ admin-service â†’ admin-pods
```

**Ingress Configuration Example**:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt"
spec:
  tls:
  - hosts:
    - app.example.com
    secretName: app-tls
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

**Key Concepts**:

1. **Ingress Controller**: Component that actually handles traffic (nginx, traefik, etc.), requires separate installation
2. **Rules**: Routing rules
   - `host`: Domain-based routing
   - `path`: Path-based routing
   - `pathType`: Exact (exact), Prefix (prefix)
3. **TLS Configuration**: Automatic HTTPS, certificates stored in Secrets
4. **Annotations**: Controller-specific configuration (rewrite, timeout, rate limiting, etc.)

#### Ingress Controller Deep Analysis

**Ingress Three Elements**:

1. **Ingress Resource**: Rule definition (only declares, doesn't handle traffic)
2. **Ingress Controller**: Rule executor (component that actually handles traffic)
3. **Ingress Class**: Controller selection (K8s 1.18+ supports multiple Controllers)

**Working Principle**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Ingress Controller Pod              â”‚
â”‚                                               â”‚
â”‚  1. Watcher                                   â”‚
â”‚     â†“ Monitors Ingress resource changes       â”‚
â”‚                                               â”‚
â”‚  2. Configuration Generator                   â”‚
â”‚     â†“ Converts Ingress rules to nginx.conf   â”‚
â”‚                                               â”‚
â”‚  3. Nginx Process                             â”‚
â”‚     â†“ Reloads config, handles requests       â”‚
â”‚                                               â”‚
â”‚  4. Backend Connections                       â”‚
â”‚     â†’ Service â†’ Pods                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Actual Deployment and Configuration**:

1. **Install Controller**:
```bash
# Method 1: Official YAML
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml

# Method 2: Helm (Recommended)
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm install nginx-ingress ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace \
  --set controller.service.type=LoadBalancer
```

2. **Verify Installation**:
```bash
# Check Controller Pod
kubectl get pods -n ingress-nginx

# Check Service
kubectl get svc -n ingress-nginx
NAME                       TYPE           EXTERNAL-IP
ingress-nginx-controller   LoadBalancer   34.102.136.180
```

3. **Advanced Configuration Example**:
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: advanced-ingress
  annotations:
    # Basic configuration
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    
    # Performance configuration
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    
    # Rate limiting configuration
    nginx.ingress.kubernetes.io/limit-rps: "10"
    nginx.ingress.kubernetes.io/limit-connections: "10"
    
    # Authentication configuration
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    
    # CORS configuration
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://frontend.example.com"
    
    # Canary deployment
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - app.example.com
    secretName: app-tls
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /api(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

**SSL/TLS Configuration**:
```bash
# Manual certificate
kubectl create secret tls tls-secret --key tls.key --cert tls.crt

# Automatic certificate (cert-manager)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - app.example.com
    secretName: app-tls-auto  # cert-manager auto-creates
```

**Monitoring and Debugging**:
```bash
# View nginx configuration
kubectl exec -n ingress-nginx <controller-pod> -- cat /etc/nginx/nginx.conf

# View access logs
kubectl logs -n ingress-nginx <controller-pod> -f

# Common troubleshooting
kubectl describe ingress my-ingress
kubectl get endpoints my-service
```

#### Ingress Controller vs NodePort Traffic Comparison

**NodePort Traffic Path**:
```
External user
    â†“
Any node IP:30080 (NodePort)
    â†“
kube-proxy (iptables/IPVS rules)
    â†“
Service (ClusterIP)
    â†“
Load balance to Pod
    â†“
Pod processes request
```

**Ingress Controller Traffic Path**:
```
External user
    â†“
Domain resolution (app.example.com â†’ LoadBalancer IP)
    â†“
LoadBalancer (cloud provider provided)
    â†“
Ingress Controller Pod
    â†“
Route based on Host/Path rules
    â†“
Corresponding Service (ClusterIP)
    â†“
Load balance to Pod
    â†“
Pod processes request
```

**Detailed Comparison Analysis**:

| Dimension | NodePort | Ingress Controller |
|-----------|----------|-------------------|
| **Number of Entry Points** | One port per Service | Unified entry (80/443) |
| **Port Management** | Limited to 30000-32767 range | Standard HTTP(S) ports |
| **Domain Support** | Not supported | Supports domain-based routing |
| **Path Routing** | Not supported | Supports path-based routing |
| **SSL Termination** | Not supported | Supports SSL/TLS termination |
| **Cost** | No additional cost | Requires LoadBalancer |
| **Production Suitability** | Development/testing | Production preferred |

**Actual Deployment Example Comparison**:

1. **NodePort Method**:
```yaml
# Need to create NodePort for each service
---
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30001  # Web service

---
apiVersion: v1
kind: Service
metadata:
  name: api-service
spec:
  type: NodePort
  selector:
    app: api
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30002  # API service

# Access methods (hard to remember ports):
# http://node-ip:30001  - Web service
# http://node-ip:30002  - API service
```

2. **Ingress Method**:
```yaml
# Services remain ClusterIP type
---
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: ClusterIP  # Internal access
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: api-service
spec:
  type: ClusterIP  # Internal access
  selector:
    app: api
  ports:
  - port: 80
    targetPort: 8080

# Unified routing through Ingress
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: unified-ingress
spec:
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80

# Access methods (standardized):
# https://app.example.com/     - Web service
# https://app.example.com/api  - API service
```

#### Relationship Between Service and Ingress Controller

**Core Relationship**: Service is the backend target of Ingress Controller

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 External Cluster Traffic            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Ingress Controller                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Routing Rule Processing              â”‚   â”‚
â”‚  â”‚  app.com/web  â†’ web-service                â”‚   â”‚
â”‚  â”‚  app.com/api  â†’ api-service                â”‚   â”‚
â”‚  â”‚  admin.com    â†’ admin-service              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“              â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚web-service  â”‚ â”‚api-service  â”‚ â”‚admin-serviceâ”‚
    â”‚(ClusterIP)  â”‚ â”‚(ClusterIP)  â”‚ â”‚(ClusterIP)  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“               â†“               â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Web Pods â”‚     â”‚API Pods â”‚     â”‚Admin Podâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Relationship Details**:

1. **Ingress Controller is a Consumer of Services**
   - Ingress Controller discovers backend Pods through Service names
   - Does not communicate directly with Pods, but with Services
   - Service provides load balancing and service discovery

2. **Service Provides Abstraction Layer for Ingress**
   - Ingress rules configure Service names, not Pod IPs
   - When Pods change, Service auto-updates, Ingress needs no modification
   - Service health checks failed Pods, Ingress won't route to failed Pods

3. **Division of Responsibilities**
   ```
   Ingress Controller responsibilities:
   - Layer 7 routing (HTTP/HTTPS)
   - Domain and path-based traffic distribution
   - SSL/TLS termination
   - Advanced features like authentication, rate limiting
   
   Service responsibilities:
   - Layer 4 load balancing (TCP/UDP)
   - Pod service discovery
   - Health checks
   - Session affinity
   ```

**Actual Workflow**:

```bash
# 1. Create Pod and Service
kubectl apply -f deployment.yaml  # Create Pod
kubectl apply -f service.yaml     # Create Service

# 2. Service auto-discovers Pod
kubectl get endpoints my-service  # View Pods discovered by Service

# 3. Create Ingress rules
kubectl apply -f ingress.yaml    # Reference Service name

# 4. Ingress Controller processing flow
External request â†’ Ingress Controller checks Host/Path
        â†“
Ingress Controller finds corresponding Service
        â†“
Resolve Service name to ClusterIP
        â†“
Service load balances to healthy Pod
        â†“
Pod processes request and returns
```

**Service Type Selection**:

When using Ingress, what type should backend Services choose?

```yaml
# âœ… Recommended: ClusterIP (default)
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: ClusterIP  # or omit (default)
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 8080

# âŒ Not recommended: NodePort (redundant exposure)
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: NodePort   # Unnecessary, Ingress already handles external access
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 8080
    nodePort: 30080

# âŒ Wrong: LoadBalancer (double load balancing)
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: LoadBalancer  # Conflicts with Ingress
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 8080
```

**Headless Service Special Case**:

In some scenarios, Ingress Controller needs direct Pod access:

```yaml
# Headless Service - directly get Pod IP list
apiVersion: v1
kind: Service
metadata:
  name: stateful-service
spec:
  clusterIP: None  # Headless Service
  selector:
    app: stateful-app
  ports:
  - port: 8080

# Ingress Controller can:
# 1. Get all Pod IP lists
# 2. Implement custom load balancing algorithms
# 3. Support session stickiness
```

**Debugging Service and Ingress Relationship**:

```bash
# 1. Check if Service is normal
kubectl get svc my-service
kubectl get endpoints my-service

# 2. Check if Ingress correctly references Service
kubectl describe ingress my-ingress

# 3. Test Service internal access
kubectl run test --image=busybox -it --rm -- wget my-service:8080

# 4. Test Ingress external access
curl -H "Host: app.example.com" http://<ingress-ip>/

# 5. View traffic flow
kubectl logs -n ingress-nginx <controller-pod> | grep my-service
```

#### Service Network Principles

**Nature of Service IP**:
```bash
# Service IP is virtual, doesn't exist on any network interface
kubectl get svc my-service
NAME         CLUSTER-IP    
my-service   10.96.10.15

# Cannot ping (no ICMP implementation)
ping 10.96.10.15  # Timeout

# But can access ports (iptables/IPVS forwarding)
curl 10.96.10.15:80  # Normal
```

**Three Modes of kube-proxy**:

1. **iptables mode** (default)
   - Kernel space forwarding, good performance
   - Performance degrades with many rules

2. **IPVS mode** (recommended)
   - Better performance (hash table vs chain rules)
   - More load balancing algorithms

3. **userspace mode** (deprecated)

**Session Affinity**:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: sticky-service
spec:
  selector:
    app: web
  sessionAffinity: ClientIP  # Same client IP routes to same Pod
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 3 hours
```

#### Service Debugging

**Troubleshooting Steps**:
```bash
# 1. Check if Pod is running
kubectl get pods -l app=web

# 2. Check Service configuration
kubectl describe svc web-service

# 3. Check Endpoints (if empty, selector doesn't match)
kubectl get endpoints web-service

# 4. Test Pod direct access
kubectl get pods -o wide  # Get Pod IP
kubectl exec test-pod -- curl 10.1.0.5:8080

# 5. Test Service access
kubectl exec test-pod -- curl web-service:80
```

### 3.3 Configuration and Storage

#### ConfigMap - Configuration Management Deep Dive

**What is ConfigMap?**
ConfigMap is a Kubernetes resource object for storing non-sensitive configuration data, achieving separation of configuration from code.

**Why Do We Need ConfigMap?**

Traditional approach problems:
```dockerfile
# âŒ Traditional approach: Configuration hardcoded in image
FROM node:14
COPY app.js /app/
COPY config.json /app/  # Configuration file packaged in image
ENV DATABASE_URL=mysql://localhost:3306/prod  # Environment variables hardcoded
CMD ["node", "/app/app.js"]
```

Problems:
- Different environments need different images
- Configuration changes require rebuilding images
- Sensitive information exposed in images

ConfigMap solution:
```
One image + Different ConfigMaps = Different environment deployments
```

**ConfigMap Creation Methods Comparison**:

1. **Command Line Creation** (suitable for quick testing):
```bash
# From file
kubectl create configmap app-config --from-file=config.properties

# From directory
kubectl create configmap app-config --from-file=./config/

# From key-value pairs
kubectl create configmap app-config \
  --from-literal=database.host=mysql.example.com \
  --from-literal=database.port=3306

# View creation result
kubectl get configmap app-config -o yaml
```

2. **YAML File Creation** (suitable for production):
```yaml
# Method 1: Key-value configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  # Simple key-value pairs
  database.host: "mysql.production.local"
  database.port: "3306"
  database.name: "myapp"
  cache.enabled: "true"
  log.level: "INFO"

---
# Method 2: File-style configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-files-config
data:
  # Complete configuration file
  database.conf: |
    [database]
    host=mysql.production.local
    port=3306
    database=myapp
    pool_size=20
    timeout=30
    
  nginx.conf: |
    server {
        listen 80;
        server_name app.example.com;
        location / {
            proxy_pass http://backend;
            proxy_set_header Host $host;
        }
    }
    
  app.properties: |
    # Application Configuration
    logging.level=INFO
    cache.enabled=true
    cache.ttl=3600
    feature.new_ui=true
```

**ConfigMap Usage Methods Deep Dive**:

1. **Environment Variable Injection**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1
        env:
        # Method 1: Single environment variable
        - name: DATABASE_HOST
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: database.host
        
        # Method 2: Batch import all keys
        envFrom:
        - configMapRef:
            name: app-config
            
        # Use within application
        command: ["sh", "-c"]
        args: ["echo Database: $DATABASE_HOST && ./start.sh"]
```

2. **File Mounting**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        volumeMounts:
        # Mount entire ConfigMap
        - name: config-volume
          mountPath: /etc/nginx/conf.d
        # Mount single file
        - name: app-config
          mountPath: /etc/app/database.conf
          subPath: database.conf
          
      volumes:
      # Mount all configuration files
      - name: config-volume
        configMap:
          name: app-files-config
      # Mount specific file
      - name: app-config
        configMap:
          name: app-files-config
          items:
          - key: database.conf
            path: database.conf
```

**Practical Application Scenarios**:

1. **Multi-environment Configuration Management**:
```yaml
# Development environment
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: development
data:
  database.host: "mysql.dev.local"
  database.name: "myapp_dev"
  log.level: "DEBUG"
  cache.enabled: "false"

# Production environment
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config  # Same name but different namespace
  namespace: production
data:
  database.host: "mysql.prod.local"
  database.name: "myapp_prod"
  log.level: "WARN"
  cache.enabled: "true"
```

2. **Application Configuration Hot Updates**:
```bash
# Update ConfigMap
kubectl patch configmap app-config -p '{"data":{"log.level":"DEBUG"}}'

# Restart Pod to apply configuration (environment variable method)
kubectl rollout restart deployment/myapp

# File mount method auto-updates (with delay, default 60 seconds)
kubectl exec -it <pod-name> -- cat /etc/config/app.properties
```

3. **Nginx Configuration Management**:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    events {
        worker_connections 1024;
    }
    http {
        upstream backend {
            server backend-service:8080;
        }
        server {
            listen 80;
            location / {
                proxy_pass http://backend;
                proxy_set_header X-Real-IP $remote_addr;
            }
        }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  template:
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: nginx-config
```

**ConfigMap Best Practices**:

1. **Naming Conventions**:
```bash
# Good naming
app-config-v1
mysql-config-production
nginx-config-frontend

# Poor naming
config
data
settings
```

2. **Version Management**:
```yaml
# Use version numbers to manage ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config-v2  # Versioned naming
  labels:
    app: myapp
    version: v2
    environment: production
data:
  config.yaml: |
    version: "2.0"
    features:
      new_ui: true
```

3. **Configuration Validation**:
```bash
# Validate syntax before creation
kubectl create configmap test-config --dry-run=client -o yaml \
  --from-file=config.yaml

# Check configuration content
kubectl describe configmap app-config
```

#### ConfigMap and SpringBoot Application Integration Deep Dive

**Real Problem**: Developers often ask "How does configuration like `database.host: "mysql.dev.local"` in ConfigMap get into SpringBoot applications? For example, how to use it in `application-prod.yaml` files?"

**Traditional SpringBoot Configuration Method**:

```yaml
# application-prod.yaml (traditional method)
spring:
  datasource:
    url: jdbc:mysql://mysql.prod.local:3306/myapp_prod
    username: ${DB_USERNAME:admin}
    password: ${DB_PASSWORD:password}
    driver-class-name: com.mysql.cj.jdbc.Driver
  
  redis:
    host: redis.prod.local
    port: 6379
    password: ${REDIS_PASSWORD}
```

**K8s + ConfigMap Integration Methods**:

**Method 1: Environment Variable Replacement (Beginner Recommended)**

1. **Create ConfigMap**:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  # Database configuration
  spring.datasource.url: "jdbc:mysql://mysql-service:3306/myapp_prod"
  spring.datasource.username: "appuser"
  spring.redis.host: "redis-service"
  spring.redis.port: "6379"
  
  # Application configuration
  app.name: "MyApp Production"
  app.version: "1.0.0"
  logging.level.com.myapp: "INFO"
```

2. **SpringBoot Application Configuration**:
```yaml
# application.yaml (in container)
spring:
  datasource:
    url: ${SPRING_DATASOURCE_URL}
    username: ${SPRING_DATASOURCE_USERNAME}
    password: ${SPRING_DATASOURCE_PASSWORD}
    driver-class-name: com.mysql.cj.jdbc.Driver
    
  redis:
    host: ${SPRING_REDIS_HOST}
    port: ${SPRING_REDIS_PORT}
    password: ${SPRING_REDIS_PASSWORD}

app:
  name: ${APP_NAME}
  version: ${APP_VERSION}

logging:
  level:
    com.myapp: ${LOGGING_LEVEL_COM_MYAPP}
```

3. **Deployment Inject Environment Variables**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springboot-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1.0
        env:
        # Inject environment variables from ConfigMap
        - name: SPRING_DATASOURCE_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: spring.datasource.url
        - name: SPRING_DATASOURCE_USERNAME
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: spring.datasource.username
        - name: SPRING_REDIS_HOST
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: spring.redis.host
        - name: APP_NAME
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: app.name
              
        # Inject sensitive information from Secret
        - name: SPRING_DATASOURCE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: spring.datasource.password
```

**Method 2: Configuration File Mounting (Production Recommended)**

1. **Complete Configuration File as ConfigMap**:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: springboot-config
data:
  application-k8s.yaml: |
    spring:
      datasource:
        url: jdbc:mysql://mysql-service:3306/myapp_prod
        username: appuser
        password: ${DB_PASSWORD}  # Still get from Secret
        driver-class-name: com.mysql.cj.jdbc.Driver
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
      
      redis:
        host: redis-service
        port: 6379
        password: ${REDIS_PASSWORD}
        timeout: 2000ms
        
      jpa:
        hibernate:
          ddl-auto: validate
        show-sql: false
          
    server:
      port: 8080
      servlet:
        context-path: /api
        
    logging:
      level:
        com.myapp: INFO
        org.springframework.security: DEBUG
        
    app:
      name: "MyApp Production"
      version: "1.0.0"
      cors:
        allowed-origins: "https://app.example.com"
```

2. **Mount Configuration File to Container**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springboot-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:v1.0
        
        # Specify SpringBoot to use k8s configuration file
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "k8s"  # Activate k8s profile
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: database.password
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: redis.password
              
        volumeMounts:
        # Mount configuration file
        - name: config-volume
          mountPath: /app/config
```

This comprehensive Part 1 covers the fundamental concepts and architecture of Kubernetes, providing a solid foundation for understanding container orchestration. The next parts will build upon these concepts with practical implementations and production-ready configurations.

---

## Series Index

- [Part 1: Fundamentals and Architecture](K8S_PART1_FUNDAMENTALS_EN.md)
- [Part 2: Configuration Management and Practical Applications](K8S_PART2_PRACTICE_EN.md)  
- [Part 3: Production Environment and Best Practices](K8S_PART3_PRODUCTION_EN.md)

*Thank you for completing this tutorial. If you found it helpful, please share it with others who might benefit.*